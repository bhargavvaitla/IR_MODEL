{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from elasticsearch import helpers, Elasticsearch\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fil = pd.read_csv('C:\\\\elastic_stack\\\\archive\\\\metadata.csv')\n",
    "#data = fil[0:1000]\n",
    "#data.to_csv('C:\\\\elastic_stack\\\\archive\\\\metadata_mini.csv')\n",
    "data = pd.read_csv('C:\\\\elastic_stack\\\\archive\\\\metadata_mini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['cord_uid','source_x','title','abstract','publish_time','authors','journal','url']]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here cord_id is unique id, so can be used as an index \n",
    "#data['cord_uid'].is_unique\n",
    "\n",
    "#data12 = data[1:15]\n",
    "data1 = data.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['cord_uid'].is_unique\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing to Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "         \"properties\": {\n",
    "                  \"cord_uid\": {\n",
    "                      \"type\": \"keyword\"\n",
    "                      },\n",
    "                  \"source_x\": {\n",
    "                      \"type\": \"keyword\",\n",
    "                  },\n",
    "                  \"title\": {\n",
    "                      \"type\": \"text\"\n",
    "                  },\n",
    "                  \"abstract\": {\n",
    "                      \"type\": \"text\",\n",
    "                  },\n",
    "                  \"publish_time\": {\n",
    "                      \"type\": \"date\",\n",
    "                  },\n",
    "                  \"authors\": {\n",
    "                      \"type\": \"text\",\n",
    "                  },\n",
    "                  \"journal\": {\n",
    "                      \"type\" :\"keyword\"\n",
    "                  },\n",
    "                  \"url\": {\n",
    "                      \"type\" :\"keyword\"\n",
    "                  }\n",
    "              }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'cord19'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=ENDPOINT)\n",
    "indexName = 'cord19'\n",
    "es.indices.delete(index=indexName, ignore=[400, 404])\n",
    "es.indices.create(index=indexName,body=mapping, ignore=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data1):\n",
    "    for ind, line in enumerate(data1):\n",
    "        yield {\n",
    "            '_index':indexName,\n",
    "            '_type':'_doc',\n",
    "            '_id':line.get(\"cord_uid\",None),\n",
    "            '_source':{\n",
    "                \"cord_uid\":line.get(\"cord_uid\",\"\"),\n",
    "                \"source_x\":line.get(\"source_x\",\"\"),\n",
    "                \"title\":line.get(\"title\",\"\"),\n",
    "                \"abstract\":line.get(\"abstract\",\"\"),\n",
    "                \"publish_time\":line.get(\"publish_time\",None),\n",
    "                \"authors\":line.get(\"authors\",\"\"),\n",
    "                \"journal\":line.get(\"journal\",\"\"),\n",
    "                \"url\":line.get(\"url\",\"\"),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp = helpers.bulk(es, generator(data1))\n",
    "    print('indexing success')\n",
    "except Exception as e:\n",
    "    print(e.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "954"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_get_all = '''{\"size\" : 1000,\"query\": {\"match_all\": {}}}'''\n",
    "results = es.search(index=indexName, body=req_get_all)['hits']['hits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = [record.get(\"_source\") for record in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_retrieved = pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, 9)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_retrieved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(text):\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove numbers \n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    #remove punctauation\n",
    "    text= text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    \n",
    "    #white space removal\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', text)\n",
    "    #Tokenization\n",
    "    tokenize_text = word_tokenize(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    text = [token for token in tokenize_text if not token in stop_words]\n",
    "    \n",
    "    #Morphological Normalization\n",
    "    stemmer = PorterStemmer()\n",
    "    text_stemmed  = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    #lem = WordNetLemmatizer()\n",
    "    #text_lem  = [lem.lemmatize(word) for word in text_stemmed]\n",
    "\n",
    "    \n",
    "    return ' '.join(text_stemmed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_retrieved['text'] = data_retrieved['title'] + data_retrieved['abstract']\n",
    "data_retrieved['text'] = data_retrieved['text'].apply(lambda x:preProcessing(x))\n",
    "#docs = data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_1 = [dat for dat in data_retrieved['text']]\n",
    "#data_retrieved.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "# just send in all your docs here\n",
    "fitted_vectorizer=tfidf_vectorizer.fit(docs)\n",
    "tfidf_vectorizer_vectors=fitted_vectorizer.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = fitted_vectorizer.get_feature_names()\n",
    "df = pd.DataFrame(list(tfidf_vectorizer_vectors.toarray()),index=data_retrieved['cord_uid'],columns=lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, 12110)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#req_get_all = '''{\"query\": {\"match_all\": {}}}'''\n",
    "#results = es.search(index=indexName, body=req_get_all)['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dis=dict(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello welcome in\n"
     ]
    }
   ],
   "source": [
    "stri = ['hello','welcome','in']\n",
    "print(' '.join(stri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
